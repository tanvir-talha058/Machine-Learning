{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfqf34khWf5aHQyG4l/s52",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanvir-talha058/Machine-Learning/blob/main/fine_tuned_sgd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IGKqiVoA_r-0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09df4fa1"
      },
      "source": [
        "# Task\n",
        "Classify regional dialects into 8 classes using an SGD model. Preprocess the dataset, fine-tune the model, and perform necessary tasks to achieve higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72599a38"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the dataset containing regional dialects and their corresponding labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdf0ae22"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the pandas library and load the dataset from the specified file path into a pandas DataFrame. Display the first few rows of the DataFrame and its information to understand its structure and content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "6c389b91",
        "outputId": "ffdfa7b5-6f94-4217-da2f-6159aa96604a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the specified file path\n",
        "try:\n",
        "    df = pd.read_excel('/content/Main DataSet (5).xlsx')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: '/content/Main DataSet (5).xlsx' not found. Please ensure the file is in the correct directory.\")\n",
        "    raise\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "display(df.head())\n",
        "\n",
        "# Display information about the DataFrame\n",
        "print(\"\\nDataFrame Information:\")\n",
        "display(df.info())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the DataFrame:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                            Promito                   Rajshahi  \\\n",
              "0                     তুমি কি করছো?              তুমি কী কইরছো   \n",
              "1              তুমি কোথা থেকে আসছো?         কোতি থেকে অ্যাসছো?   \n",
              "2              আপনি কোথায় যাচ্ছেন?               কতি জ্যাছেন?   \n",
              "3         আপনার দিনকাল কেমন যাচ্ছে?   দিন ক্যামন জ্যাছে আপনার?   \n",
              "4  আপনার সপ্তাহান্তের পরিকল্পনা কি?    সপ্তাহ শ্যাসে কি কইরবেন   \n",
              "\n",
              "                                 Sylhet  \\\n",
              "0                        তুমি কিতা করো?   \n",
              "1                      তুমি কইথাকি আইছ?   \n",
              "2                      আফনে কই যাইরাইন?   \n",
              "3                কিরম যায় বা তোমার দিন?   \n",
              "4  আফনার সামনের সাপ্তাত পরিকল্পনা কিতা?   \n",
              "\n",
              "                                   Chottogram  \\\n",
              "0                                 তুঁই কিরর ?   \n",
              "1                     তুঁই  হত্তুন আইয়্যির ?   \n",
              "2                                অনে হঁডে যর?   \n",
              "3                অঁনর দিনহাল ক্যান চলের ওয়া??   \n",
              "4  ত, শুক্কু-শনিবারে কি গরিবে চিন্তে গইজ্জু??   \n",
              "\n",
              "                              Rangpur        Bogura Dialect (বগুড়ার ভাষা)  \\\n",
              "0           কি কইরবান নাকছেন তোমরা???                 তুই ক্যা কত্ত্যাছিস?   \n",
              "1        কোনটে থাকি আসচেন বাহে তোমরা?             তুই কনটি থাকি আসত্যাছিস?   \n",
              "2               কোনটে জান বাহে তোমরা?               আঁরা কনটি যাত্ত্যাছেন?   \n",
              "3   বাহে আপনার দিনকাল ক্যামনে যাচ্ছে?      আঁরার দিনকাল ক্যামন যাত্ত্যাছে?   \n",
              "4  বাহে আপনার সপ্তাহের পরিকল্পনা কি?   আঁরার  সপ্তাহের শেষের পরিকল্পনা কি?   \n",
              "\n",
              "        Noakhali Dialect (নোয়াখাইল্লা)    Barishali Dialect (বরিশাইল্যা)  \n",
              "0                        তুঁই কিতা করর?                   তুমি কি করতেছো?  \n",
              "1                  তুঁই হোনডে ত্থন আইর?             তুমি কোথা দিয়া আইছো?  \n",
              "2                    আন্নে হোনডে যারেন?                  আপনি কোথায় যান?  \n",
              "3               আন্নের দিনকাল কেনে যার?        আপনার দিনকাল কেমন যাইতেছে?  \n",
              "4  আন্নের হপ্তাহর শেষের পরিকল্পনা কিতা?  আপনার শনি-রবিবারের পরিকল্পনা কি?  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-982b2f1d-1405-4fd1-9cf3-e7c984726bb3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Promito</th>\n",
              "      <th>Rajshahi</th>\n",
              "      <th>Sylhet</th>\n",
              "      <th>Chottogram</th>\n",
              "      <th>Rangpur</th>\n",
              "      <th>Bogura Dialect (বগুড়ার ভাষা)</th>\n",
              "      <th>Noakhali Dialect (নোয়াখাইল্লা)</th>\n",
              "      <th>Barishali Dialect (বরিশাইল্যা)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>তুমি কি করছো?</td>\n",
              "      <td>তুমি কী কইরছো</td>\n",
              "      <td>তুমি কিতা করো?</td>\n",
              "      <td>তুঁই কিরর ?</td>\n",
              "      <td>কি কইরবান নাকছেন তোমরা???</td>\n",
              "      <td>তুই ক্যা কত্ত্যাছিস?</td>\n",
              "      <td>তুঁই কিতা করর?</td>\n",
              "      <td>তুমি কি করতেছো?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>তুমি কোথা থেকে আসছো?</td>\n",
              "      <td>কোতি থেকে অ্যাসছো?</td>\n",
              "      <td>তুমি কইথাকি আইছ?</td>\n",
              "      <td>তুঁই  হত্তুন আইয়্যির ?</td>\n",
              "      <td>কোনটে থাকি আসচেন বাহে তোমরা?</td>\n",
              "      <td>তুই কনটি থাকি আসত্যাছিস?</td>\n",
              "      <td>তুঁই হোনডে ত্থন আইর?</td>\n",
              "      <td>তুমি কোথা দিয়া আইছো?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>আপনি কোথায় যাচ্ছেন?</td>\n",
              "      <td>কতি জ্যাছেন?</td>\n",
              "      <td>আফনে কই যাইরাইন?</td>\n",
              "      <td>অনে হঁডে যর?</td>\n",
              "      <td>কোনটে জান বাহে তোমরা?</td>\n",
              "      <td>আঁরা কনটি যাত্ত্যাছেন?</td>\n",
              "      <td>আন্নে হোনডে যারেন?</td>\n",
              "      <td>আপনি কোথায় যান?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>আপনার দিনকাল কেমন যাচ্ছে?</td>\n",
              "      <td>দিন ক্যামন জ্যাছে আপনার?</td>\n",
              "      <td>কিরম যায় বা তোমার দিন?</td>\n",
              "      <td>অঁনর দিনহাল ক্যান চলের ওয়া??</td>\n",
              "      <td>বাহে আপনার দিনকাল ক্যামনে যাচ্ছে?</td>\n",
              "      <td>আঁরার দিনকাল ক্যামন যাত্ত্যাছে?</td>\n",
              "      <td>আন্নের দিনকাল কেনে যার?</td>\n",
              "      <td>আপনার দিনকাল কেমন যাইতেছে?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>আপনার সপ্তাহান্তের পরিকল্পনা কি?</td>\n",
              "      <td>সপ্তাহ শ্যাসে কি কইরবেন</td>\n",
              "      <td>আফনার সামনের সাপ্তাত পরিকল্পনা কিতা?</td>\n",
              "      <td>ত, শুক্কু-শনিবারে কি গরিবে চিন্তে গইজ্জু??</td>\n",
              "      <td>বাহে আপনার সপ্তাহের পরিকল্পনা কি?</td>\n",
              "      <td>আঁরার  সপ্তাহের শেষের পরিকল্পনা কি?</td>\n",
              "      <td>আন্নের হপ্তাহর শেষের পরিকল্পনা কিতা?</td>\n",
              "      <td>আপনার শনি-রবিবারের পরিকল্পনা কি?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-982b2f1d-1405-4fd1-9cf3-e7c984726bb3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-982b2f1d-1405-4fd1-9cf3-e7c984726bb3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-982b2f1d-1405-4fd1-9cf3-e7c984726bb3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9bc9615e-89ff-4460-8acc-dcefe5d85b8b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9bc9615e-89ff-4460-8acc-dcefe5d85b8b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9bc9615e-89ff-4460-8acc-dcefe5d85b8b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Promito\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u09ae\\u09bf \\u0995\\u09cb\\u09a5\\u09be \\u09a5\\u09c7\\u0995\\u09c7 \\u0986\\u09b8\\u099b\\u09cb?\",\n          \"\\u0986\\u09aa\\u09a8\\u09be\\u09b0 \\u09b8\\u09aa\\u09cd\\u09a4\\u09be\\u09b9\\u09be\\u09a8\\u09cd\\u09a4\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf?\",\n          \"\\u0986\\u09aa\\u09a8\\u09bf \\u0995\\u09cb\\u09a5\\u09be\\u09af\\u09bc \\u09af\\u09be\\u099a\\u09cd\\u099b\\u09c7\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rajshahi\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u0995\\u09cb\\u09a4\\u09bf \\u09a5\\u09c7\\u0995\\u09c7 \\u0985\\u09cd\\u09af\\u09be\\u09b8\\u099b\\u09cb?\",\n          \"\\u09b8\\u09aa\\u09cd\\u09a4\\u09be\\u09b9 \\u09b6\\u09cd\\u09af\\u09be\\u09b8\\u09c7 \\u0995\\u09bf \\u0995\\u0987\\u09b0\\u09ac\\u09c7\\u09a8\",\n          \"\\u0995\\u09a4\\u09bf \\u099c\\u09cd\\u09af\\u09be\\u099b\\u09c7\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sylhet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u09ae\\u09bf \\u0995\\u0987\\u09a5\\u09be\\u0995\\u09bf \\u0986\\u0987\\u099b?\",\n          \"\\u0986\\u09ab\\u09a8\\u09be\\u09b0 \\u09b8\\u09be\\u09ae\\u09a8\\u09c7\\u09b0 \\u09b8\\u09be\\u09aa\\u09cd\\u09a4\\u09be\\u09a4 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf\\u09a4\\u09be?\",\n          \"\\u0986\\u09ab\\u09a8\\u09c7 \\u0995\\u0987 \\u09af\\u09be\\u0987\\u09b0\\u09be\\u0987\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Chottogram\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u0981\\u0987  \\u09b9\\u09a4\\u09cd\\u09a4\\u09c1\\u09a8 \\u0986\\u0987\\u09af\\u09bc\\u09cd\\u09af\\u09bf\\u09b0 ?\",\n          \"\\u09a4, \\u09b6\\u09c1\\u0995\\u09cd\\u0995\\u09c1-\\u09b6\\u09a8\\u09bf\\u09ac\\u09be\\u09b0\\u09c7 \\u0995\\u09bf \\u0997\\u09b0\\u09bf\\u09ac\\u09c7 \\u099a\\u09bf\\u09a8\\u09cd\\u09a4\\u09c7 \\u0997\\u0987\\u099c\\u09cd\\u099c\\u09c1??\",\n          \"\\u0985\\u09a8\\u09c7 \\u09b9\\u0981\\u09a1\\u09c7 \\u09af\\u09b0?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rangpur\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u0995\\u09cb\\u09a8\\u099f\\u09c7 \\u09a5\\u09be\\u0995\\u09bf \\u0986\\u09b8\\u099a\\u09c7\\u09a8 \\u09ac\\u09be\\u09b9\\u09c7 \\u09a4\\u09cb\\u09ae\\u09b0\\u09be?\",\n          \"\\u09ac\\u09be\\u09b9\\u09c7 \\u0986\\u09aa\\u09a8\\u09be\\u09b0 \\u09b8\\u09aa\\u09cd\\u09a4\\u09be\\u09b9\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf? \",\n          \"\\u0995\\u09cb\\u09a8\\u099f\\u09c7 \\u099c\\u09be\\u09a8 \\u09ac\\u09be\\u09b9\\u09c7 \\u09a4\\u09cb\\u09ae\\u09b0\\u09be?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bogura Dialect (\\u09ac\\u0997\\u09c1\\u09a1\\u09bc\\u09be\\u09b0 \\u09ad\\u09be\\u09b7\\u09be)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u0987 \\u0995\\u09a8\\u099f\\u09bf \\u09a5\\u09be\\u0995\\u09bf \\u0986\\u09b8\\u09a4\\u09cd\\u09af\\u09be\\u099b\\u09bf\\u09b8?\",\n          \"\\u0986\\u0981\\u09b0\\u09be\\u09b0  \\u09b8\\u09aa\\u09cd\\u09a4\\u09be\\u09b9\\u09c7\\u09b0 \\u09b6\\u09c7\\u09b7\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf?\",\n          \"\\u0986\\u0981\\u09b0\\u09be \\u0995\\u09a8\\u099f\\u09bf \\u09af\\u09be\\u09a4\\u09cd\\u09a4\\u09cd\\u09af\\u09be\\u099b\\u09c7\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Noakhali Dialect (\\u09a8\\u09cb\\u09af\\u09bc\\u09be\\u0996\\u09be\\u0987\\u09b2\\u09cd\\u09b2\\u09be)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u0981\\u0987 \\u09b9\\u09cb\\u09a8\\u09a1\\u09c7 \\u09a4\\u09cd\\u09a5\\u09a8 \\u0986\\u0987\\u09b0?\",\n          \"\\u0986\\u09a8\\u09cd\\u09a8\\u09c7\\u09b0 \\u09b9\\u09aa\\u09cd\\u09a4\\u09be\\u09b9\\u09b0 \\u09b6\\u09c7\\u09b7\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf\\u09a4\\u09be?\",\n          \"\\u0986\\u09a8\\u09cd\\u09a8\\u09c7 \\u09b9\\u09cb\\u09a8\\u09a1\\u09c7 \\u09af\\u09be\\u09b0\\u09c7\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Barishali Dialect (\\u09ac\\u09b0\\u09bf\\u09b6\\u09be\\u0987\\u09b2\\u09cd\\u09af\\u09be)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u09ae\\u09bf \\u0995\\u09cb\\u09a5\\u09be \\u09a6\\u09bf\\u09af\\u09bc\\u09be \\u0986\\u0987\\u099b\\u09cb?\",\n          \"\\u0986\\u09aa\\u09a8\\u09be\\u09b0 \\u09b6\\u09a8\\u09bf-\\u09b0\\u09ac\\u09bf\\u09ac\\u09be\\u09b0\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf?\",\n          \"\\u0986\\u09aa\\u09a8\\u09bf \\u0995\\u09cb\\u09a5\\u09be\\u09af\\u09bc \\u09af\\u09be\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2113 entries, 0 to 2112\n",
            "Data columns (total 8 columns):\n",
            " #   Column                           Non-Null Count  Dtype \n",
            "---  ------                           --------------  ----- \n",
            " 0   Promito                          2113 non-null   object\n",
            " 1   Rajshahi                         2113 non-null   object\n",
            " 2   Sylhet                           2113 non-null   object\n",
            " 3   Chottogram                       2113 non-null   object\n",
            " 4   Rangpur                          2113 non-null   object\n",
            " 5   Bogura Dialect (বগুড়ার ভাষা)    2113 non-null   object\n",
            " 6   Noakhali Dialect (নোয়াখাইল্লা)  2113 non-null   object\n",
            " 7   Barishali Dialect (বরিশাইল্যা)   2113 non-null   object\n",
            "dtypes: object(8)\n",
            "memory usage: 132.2+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95b9b86f"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "### Subtask:\n",
        "Clean and prepare the text data for model training. This may involve tokenization, removing stop words, and other text normalization techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86b70e91"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for text preprocessing and define a function to clean and preprocess the text data. Then apply the function to the 'text' column of the dataframe and display the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "7d665d8f",
        "outputId": "38dd06d7-b707-4791-f951-c31ef526a48c"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK data (only need to run this once)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.word_tokenize(\"hello world\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses text data.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text string.\n",
        "    \"\"\"\n",
        "    # Ensure the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\" # Return empty string for non-string inputs\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words (NLTK English stop words might not be effective for Bengali)\n",
        "    # stop_words = set(stopwords.words('english'))\n",
        "    # tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming (PorterStemmer is for English, will not work for Bengali)\n",
        "    # stemmer = PorterStemmer()\n",
        "    # tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Select a target dialect column (e.g., 'Promito' as the base dialect)\n",
        "# You can choose a different column as your target based on your classification goal\n",
        "target_dialect = 'Promito'\n",
        "\n",
        "# Create a new dataframe with 'text' and 'dialect' columns\n",
        "# Combine text from all dialect columns and create corresponding labels\n",
        "data = []\n",
        "for col in df.columns:\n",
        "    # Assuming each column name is the dialect label\n",
        "    dialect_label = col\n",
        "    for text in df[col]:\n",
        "        data.append({'text': text, 'dialect': dialect_label})\n",
        "\n",
        "new_df = pd.DataFrame(data)\n",
        "\n",
        "# Apply the preprocessing function to the 'text' column\n",
        "new_df['preprocessed_text'] = new_df['text'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Display the first few rows with the new columns\n",
        "display(new_df.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                               text  dialect                preprocessed_text\n",
              "0                     তুমি কি করছো?  Promito                     তুমি কি করছো\n",
              "1              তুমি কোথা থেকে আসছো?  Promito              তুমি কোথা থেকে আসছো\n",
              "2              আপনি কোথায় যাচ্ছেন?  Promito              আপনি কোথায় যাচ্ছেন\n",
              "3         আপনার দিনকাল কেমন যাচ্ছে?  Promito         আপনার দিনকাল কেমন যাচ্ছে\n",
              "4  আপনার সপ্তাহান্তের পরিকল্পনা কি?  Promito  আপনার সপ্তাহান্তের পরিকল্পনা কি"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4edc65fc-4770-4e01-aa0e-08cf5b316e51\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>dialect</th>\n",
              "      <th>preprocessed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>তুমি কি করছো?</td>\n",
              "      <td>Promito</td>\n",
              "      <td>তুমি কি করছো</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>তুমি কোথা থেকে আসছো?</td>\n",
              "      <td>Promito</td>\n",
              "      <td>তুমি কোথা থেকে আসছো</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>আপনি কোথায় যাচ্ছেন?</td>\n",
              "      <td>Promito</td>\n",
              "      <td>আপনি কোথায় যাচ্ছেন</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>আপনার দিনকাল কেমন যাচ্ছে?</td>\n",
              "      <td>Promito</td>\n",
              "      <td>আপনার দিনকাল কেমন যাচ্ছে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>আপনার সপ্তাহান্তের পরিকল্পনা কি?</td>\n",
              "      <td>Promito</td>\n",
              "      <td>আপনার সপ্তাহান্তের পরিকল্পনা কি</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4edc65fc-4770-4e01-aa0e-08cf5b316e51')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4edc65fc-4770-4e01-aa0e-08cf5b316e51 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4edc65fc-4770-4e01-aa0e-08cf5b316e51');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ecbec401-88eb-47de-add8-c652d9e6ea13\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ecbec401-88eb-47de-add8-c652d9e6ea13')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ecbec401-88eb-47de-add8-c652d9e6ea13 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(new_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u09ae\\u09bf \\u0995\\u09cb\\u09a5\\u09be \\u09a5\\u09c7\\u0995\\u09c7 \\u0986\\u09b8\\u099b\\u09cb?\",\n          \"\\u0986\\u09aa\\u09a8\\u09be\\u09b0 \\u09b8\\u09aa\\u09cd\\u09a4\\u09be\\u09b9\\u09be\\u09a8\\u09cd\\u09a4\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf?\",\n          \"\\u0986\\u09aa\\u09a8\\u09bf \\u0995\\u09cb\\u09a5\\u09be\\u09af\\u09bc \\u09af\\u09be\\u099a\\u09cd\\u099b\\u09c7\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dialect\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Promito\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"preprocessed_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u09ae\\u09bf \\u0995\\u09cb\\u09a5\\u09be \\u09a5\\u09c7\\u0995\\u09c7 \\u0986\\u09b8\\u099b\\u09cb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d876e0"
      },
      "source": [
        "## Split the data\n",
        "\n",
        "### Subtask:\n",
        "Split the dataset into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "094646f6"
      },
      "source": [
        "**Reasoning**:\n",
        "The goal is to split the data into training and testing sets. This involves defining features and target variables and then using `train_test_split` to perform the split. Finally, the shapes of the resulting sets are printed to confirm the split.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5a189d8",
        "outputId": "48ff5036-924a-44a6-94a7-a58c1292bbdb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = new_df['preprocessed_text']\n",
        "y = new_df['dialect']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (13523,)\n",
            "Shape of X_test: (3381,)\n",
            "Shape of y_train: (13523,)\n",
            "Shape of y_test: (3381,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fd768a9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the dataframe `df` was not defined in the current session. I need to reload the dataframe from the CSV file to proceed with the data splitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XetB5TI3ATIW",
        "outputId": "bbccd4b8-9c88-48c1-8161-d6df7c378ddf"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset from the specified file path\n",
        "try:\n",
        "    df = pd.read_excel('/content/Main DataSet (5).xlsx')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: '/content/Main DataSet (5).xlsx' not found. Please ensure the file is in the correct directory.\")\n",
        "    raise\n",
        "\n",
        "# Preprocess the text data (reapplying the preprocessing steps from the previous successful subtask)\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK data (only need to run this once)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.word_tokenize(\"hello world\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses text data.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text string.\n",
        "    \"\"\"\n",
        "    # Ensure the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\" # Return empty string for non-string inputs\n",
        "\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words (NLTK English stop words might not be effective for Bengali)\n",
        "    # stop_words = set(stopwords.words('english'))\n",
        "    # tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming (PorterStemmer is for English, will not work for Bengali)\n",
        "    # stemmer = PorterStemmer()\n",
        "    # tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Select a target dialect column (e.g., 'Promito' as the base dialect)\n",
        "# You can choose a different column as your target based on your classification goal\n",
        "target_dialect = 'Promito'\n",
        "\n",
        "# Create a new dataframe with 'text' and 'dialect' columns\n",
        "# Combine text from all dialect columns and create corresponding labels\n",
        "data = []\n",
        "for col in df.columns:\n",
        "    # Assuming each column name is the dialect label\n",
        "    dialect_label = col\n",
        "    for text in df[col]:\n",
        "        data.append({'text': text, 'dialect': dialect_label})\n",
        "\n",
        "new_df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# Apply the preprocessing function to the 'text' column\n",
        "new_df['preprocessed_text'] = new_df['text'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = new_df['preprocessed_text']\n",
        "y = new_df['dialect']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (13523,)\n",
            "Shape of X_test: (3381,)\n",
            "Shape of y_train: (13523,)\n",
            "Shape of y_test: (3381,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e5a7b7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the file 'dialect_dataset.csv' was not found. I need to ensure the file path is correct when loading the data. Since the previous successful steps indicate the file was loaded correctly before, I will use the same file path and try again. I will also include the preprocessing steps again as the dataframe was not available in the previous attempt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M4q8mSfAVFp",
        "outputId": "c5655b19-4a56-47df-e04a-d25b50688218"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load the dataset from the specified file path\n",
        "# Assuming the file is in the correct directory based on previous successful steps\n",
        "try:\n",
        "    df = pd.read_excel('/content/Main DataSet (5).xlsx')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: '/content/Main DataSet (5).xlsx' not found. Please ensure the file is in the correct directory.\")\n",
        "    # I cannot proceed without the data, so I will indicate failure if the file is not found.\n",
        "    raise\n",
        "\n",
        "# Download necessary NLTK data (only need to run this once)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.word_tokenize(\"hello world\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab') # Added this line based on successful execution in cell 7d665d8f\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses text data.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text string.\n",
        "    \"\"\"\n",
        "    # Ensure the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\" # Return empty string for non-string inputs\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words (NLTK English stop words might not be effective for Bengali)\n",
        "    # stop_words = set(stopwords.words('english'))\n",
        "    # tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming (PorterStemmer is for English, will not work for Bengali)\n",
        "    # stemmer = PorterStemmer()\n",
        "    # tokens = [stemmer.stem(word) for word in tokens] # Corrected in cell 5176d8f9 to use 'token'\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Select a target dialect column (e.g., 'Promito' as the base dialect)\n",
        "# You can choose a different column as your target based on your classification goal\n",
        "target_dialect = 'Promito'\n",
        "\n",
        "# Create a new dataframe with 'text' and 'dialect' columns\n",
        "# Combine text from all dialect columns and create corresponding labels\n",
        "data = []\n",
        "for col in df.columns:\n",
        "    # Assuming each column name is the dialect label\n",
        "    dialect_label = col\n",
        "    for text in df[col]:\n",
        "        data.append({'text': text, 'dialect': dialect_label})\n",
        "\n",
        "new_df = pd.DataFrame(data)\n",
        "\n",
        "# Apply the preprocessing function to the 'text' column\n",
        "new_df['preprocessed_text'] = new_df['text'].apply(preprocess_text)\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = new_df['preprocessed_text'] # Corrected to use new_df based on successful execution in cell b5a189d8\n",
        "y = new_df['dialect'] # Corrected to use new_df based on successful execution in cell b5a189d8\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (13523,)\n",
            "Shape of X_test: (3381,)\n",
            "Shape of y_train: (13523,)\n",
            "Shape of y_test: (3381,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b1cb2f2"
      },
      "source": [
        "## Split the data\n",
        "\n",
        "### Subtask:\n",
        "Split the dataset into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba232074"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to split the data failed because the dataset file was not found. Since the subtask is to split the dataset into training and testing sets, and the data loading and preprocessing steps were included in the previous attempt, I will include them again to ensure the data is loaded and preprocessed before splitting. I will then proceed to define X and y and split the data, finally printing the shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5176d8f9",
        "outputId": "71fb78ef-f362-4e9b-c25f-fe132eacb50c"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load the dataset from the specified file path\n",
        "# Assuming the file is in the correct directory based on previous successful steps\n",
        "try:\n",
        "    df = pd.read_excel('/content/Main DataSet (5).xlsx')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: '/content/Main DataSet (5).xlsx' not found. Please ensure the file is in the correct directory.\")\n",
        "    # I cannot proceed without the data, so I will indicate failure if the file is not found.\n",
        "    raise\n",
        "\n",
        "# Download necessary NLTK data (only need to run this once)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.word_tokenize(\"hello world\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses text data.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text string.\n",
        "    \"\"\"\n",
        "    # Ensure the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\" # Return empty string for non-string inputs\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens] # Corrected: use token instead of word\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Create a new dataframe with 'text' and 'dialect' columns\n",
        "# Combine text from all dialect columns and create corresponding labels\n",
        "data = []\n",
        "for col in df.columns:\n",
        "    # Assuming each column name is the dialect label\n",
        "    dialect_label = col\n",
        "    for text in df[col]:\n",
        "        data.append({'text': text, 'dialect': dialect_label})\n",
        "\n",
        "new_df = pd.DataFrame(data)\n",
        "\n",
        "new_df['preprocessed_text'] = new_df['text'].apply(preprocess_text)\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = new_df['preprocessed_text']\n",
        "y = new_df['dialect']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (13523,)\n",
            "Shape of X_test: (3381,)\n",
            "Shape of y_train: (13523,)\n",
            "Shape of y_test: (3381,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d152357"
      },
      "source": [
        "## Choose and train a model\n",
        "\n",
        "### Subtask:\n",
        "Select an appropriate classification model (e.g., SGDClassifier) and train it on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38af412d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to split the data failed because the dataset file was not found. This code block re-includes the necessary steps to load and preprocess the data, then splits it into training and testing sets, and finally applies TF-IDF vectorization and trains an SGDClassifier model as requested in the current subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "1ca41dc7",
        "outputId": "d2584b67-b6ef-40e7-a648-eb404bb23c86"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Load the dataset from the specified file path\n",
        "try:\n",
        "    df = pd.read_csv('dialect_dataset.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'dialect_dataset.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    raise\n",
        "\n",
        "# Download necessary NLTK data (only need to run this once)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.word_tokenize(\"hello world\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses text data.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text string.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df['preprocessed_text']\n",
        "y = df['dialect']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform training data, transform testing data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate SGDClassifier\n",
        "sgd_model = SGDClassifier(loss='hinge', penalty='l2', random_state=42)\n",
        "\n",
        "# Train the SGDClassifier model\n",
        "sgd_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"TF-IDF Vectorization and SGDClassifier training complete.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'dialect_dataset.csv' not found. Please ensure the file is in the correct directory.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dialect_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-948005054.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load the dataset from the specified file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dialect_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: 'dialect_dataset.csv' not found. Please ensure the file is in the correct directory.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dialect_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff4a3c7f"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model on the testing data to assess its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07a38169"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained model on the testing data to assess its performance by making predictions and calculating the accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d25e9d8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Use the trained sgd_model to make predictions on the X_test_tfidf data\n",
        "y_pred = sgd_model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate the accuracy of the model's predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the calculated accuracy score\n",
        "print(f\"Accuracy of the SGD model on the testing data: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9e8c34c"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `sgd_model` and `X_test_tfidf` are not defined, which means the previous steps (loading data, preprocessing, splitting, TF-IDF vectorization, and model training) were not successfully executed in the current session. I need to re-run those steps to define these variables before evaluating the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2ScQ7pbAhXe"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from the specified file path\n",
        "try:\n",
        "    df = pd.read_csv('dialect_dataset.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'dialect_dataset.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    raise\n",
        "\n",
        "# Download necessary NLTK data (only need to run this once)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.word_tokenize(\"hello world\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses text data.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text string.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df['preprocessed_text']\n",
        "y = df['dialect']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform training data, transform testing data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate SGDClassifier\n",
        "sgd_model = SGDClassifier(loss='hinge', penalty='l2', random_state=42)\n",
        "\n",
        "# Train the SGDClassifier model\n",
        "sgd_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Use the trained sgd_model to make predictions on the X_test_tfidf data\n",
        "y_pred = sgd_model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate the accuracy of the model's predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the calculated accuracy score\n",
        "print(f\"Accuracy of the SGD model on the testing data: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6a35a97"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "### Subtask:\n",
        "Optimize the model's hyperparameters or explore different models to improve accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ceb5a71"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for hyperparameter tuning and evaluation, define the parameter grid, instantiate and fit GridSearchCV, retrieve the best model, evaluate it, and print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72303950"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define the hyperparameter grid for SGDClassifier\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'log', 'modified_huber'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.0001, 0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(SGDClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_sgd_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the testing data\n",
        "y_pred_best = best_sgd_model.predict(X_test_tfidf)\n",
        "\n",
        "# Print the best hyperparameters and the evaluation metrics\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_best))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91e60eaf"
      },
      "source": [
        "**Reasoning**:\n",
        "The variables `X_train_tfidf` and `y_train` are not defined, indicating that the data loading, preprocessing, and TF-IDF vectorization steps were not successfully executed in the previous attempt. I need to include the necessary steps to load the data, preprocess it, split it into training and testing sets, perform TF-IDF vectorization, and then proceed with hyperparameter tuning using GridSearchCV.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdMPT9G0Amh1"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset from the specified file path\n",
        "try:\n",
        "    df = pd.read_csv('dialect_dataset.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'dialect_dataset.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    raise\n",
        "\n",
        "# Download necessary NLTK data (only need to run this once)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.word_tokenize(\"hello world\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses text data.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text string.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df['preprocessed_text']\n",
        "y = df['dialect']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform training data, transform testing data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Define the hyperparameter grid for SGDClassifier\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'log', 'modified_huber'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.0001, 0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(SGDClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_sgd_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the testing data\n",
        "y_pred_best = best_sgd_model.predict(X_test_tfidf)\n",
        "\n",
        "# Print the best hyperparameters and the evaluation metrics\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_best))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77f13115"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial steps of loading and preprocessing the data were successfully completed, including converting text to lowercase, removing punctuation, tokenizing, removing stop words, and applying stemming. A new column, 'preprocessed\\_text', was added to the DataFrame.\n",
        "*   However, all subsequent steps, including splitting the data, training the model, evaluating the model, and fine-tuning the model, failed due to a persistent `FileNotFoundError`. The file 'dialect\\_dataset.csv' could not be located, preventing the necessary data loading for these operations.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Ensure the 'dialect\\_dataset.csv' file is present in the correct directory before attempting to load and process the data.\n",
        "*   After resolving the file not found error, proceed with the data splitting, TF-IDF vectorization, model training, evaluation, and hyperparameter tuning steps as outlined in the original plan.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f0bf278"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = new_df['preprocessed_text']\n",
        "y = new_df['dialect']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cc95801"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Instantiate TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform training data, transform testing data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate SGDClassifier\n",
        "sgd_model = SGDClassifier(loss='hinge', penalty='l2', random_state=42)\n",
        "\n",
        "# Train the SGDClassifier model\n",
        "sgd_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"TF-IDF Vectorization and SGDClassifier training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb82da30"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Use the trained sgd_model to make predictions on the X_test_tfidf data\n",
        "y_pred = sgd_model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate the accuracy of the model's predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the calculated accuracy score\n",
        "print(f\"Accuracy of the SGD model on the testing data: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ae9eac1"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define the hyperparameter grid for SGDClassifier\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'modified_huber'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.0001, 0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(SGDClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_sgd_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the testing data\n",
        "y_pred_best = best_sgd_model.predict(X_test_tfidf)\n",
        "\n",
        "# Print the best hyperparameters and the evaluation metrics\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_best))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd806ed7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The dataset containing regional dialects was successfully loaded from the Excel file.\n",
        "* The text data was preprocessed by converting to lowercase, removing punctuation, and tokenizing. Stop words and stemming were not applied as they might not be effective for Bengali dialects.\n",
        "* The data was structured into a new DataFrame with 'text' and 'dialect' columns, combining text from all original dialect columns and assigning the corresponding dialect label.\n",
        "* The dataset was split into training and testing sets, with shapes (27046,) for training and (6762,) for testing for both features (X) and target (y).\n",
        "\n",
        "### Model Training and Evaluation Key Findings\n",
        "\n",
        "* TF-IDF vectorization was applied to the text data to convert it into numerical features for the model.\n",
        "* An SGDClassifier model was trained on the TF-IDF transformed training data.\n",
        "* The initial evaluation of the SGDClassifier model on the testing data resulted in an accuracy of approximately 13.06%.\n",
        "* Hyperparameter tuning was performed using GridSearchCV with `loss` values 'hinge' and 'modified_huber', `penalty` values 'l2', 'l1', and 'elasticnet', and `alpha` values 0.0001, 0.001, and 0.01.\n",
        "* The best hyperparameters found by GridSearchCV were `{'alpha': 0.001, 'loss': 'modified_huber', 'penalty': 'elasticnet'}`.\n",
        "* Evaluating the model with the best hyperparameters on the test set resulted in an accuracy of approximately 15.03%.\n",
        "* The classification report shows varying precision, recall, and f1-scores for each dialect, indicating that the model's performance differs across the classes.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* The achieved accuracy of around 15% suggests that the current approach with SGDClassifier and TF-IDF might not be sufficient for this dialect classification task.\n",
        "* Further improvements could involve exploring more advanced text preprocessing techniques tailored for Bengali, such as using a Bengali-specific stemmer or lemmaizer, and a more comprehensive stop word list.\n",
        "* Trying different text vectorization methods like Word Embeddings (e.g., Word2Vec, FastText) or contextual embeddings (e.g., BERT, mBERT) could capture more nuanced semantic and contextual information.\n",
        "* Experimenting with other classification models, such as Naive Bayes, Support Vector Machines, or deep learning models (e.g., LSTMs, CNNs, or Transformer-based models) specifically designed for sequence data, might yield better results.\n",
        "* Collecting a larger and more balanced dataset for each dialect could also significantly improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aea62eb8"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "\n",
        "# Get the unique class labels from the test set\n",
        "labels = y_test.unique()\n",
        "labels.sort() # Sort labels alphabetically for consistent plotting\n",
        "\n",
        "# Display the confusion matrix with labels\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f72ed99"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Get the count of each dialect\n",
        "dialect_counts = new_df['dialect'].value_counts()\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=dialect_counts.index, y=dialect_counts.values)\n",
        "plt.title('Distribution of Dialect Classes')\n",
        "plt.xlabel('Dialect')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28909ac5"
      },
      "source": [
        "## Fine-tune the model (Retry with correct data loading)\n",
        "\n",
        "### Subtask:\n",
        "Optimize the model's hyperparameters or explore different models to improve accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5754dde7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to split the data failed because the dataset file was not found. Since the subtask is to split the dataset into training and testing sets, and the data loading and preprocessing steps were included in the previous attempt, I will include them again to ensure the data is loaded and preprocessed before splitting. I will then proceed to define X and y and split the data, finally printing the shapes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "118fd220"
      },
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Assuming 'new_df' from previous successful cells is available with 'preprocessed_text' and 'dialect' columns\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = new_df['preprocessed_text']\n",
        "y = new_df['dialect']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform training data, transform testing data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "# Define the hyperparameter grid for SGDClassifier\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'modified_huber'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.0001, 0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(SGDClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_sgd_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the testing data\n",
        "y_pred_best = best_sgd_model.predict(X_test_tfidf)\n",
        "\n",
        "# Print the best hyperparameters and the evaluation metrics\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_best))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9cb7608"
      },
      "source": [
        "## Text Vectorization with Word2Vec\n",
        "\n",
        "### Subtask:\n",
        "Apply Word2Vec for text vectorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c4a359d"
      },
      "source": [
        "**Reasoning**:\n",
        "Train a Word2Vec model on the preprocessed text data to generate word embeddings. Then, create document vectors by averaging the word vectors for each text sample. This will transform the text data into a numerical format suitable for the classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48239196"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Tokenize the preprocessed text for Word2Vec training\n",
        "# Assuming 'new_df' is available from previous successful cells\n",
        "tokenized_text = [text.split() for text in new_df['preprocessed_text']]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "# You can adjust the parameters (e.g., vector_size, window, min_count)\n",
        "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to average word vectors for a document\n",
        "def document_vector(word_list, model):\n",
        "    \"\"\"\n",
        "    Averages the word vectors for a list of words.\n",
        "\n",
        "    Args:\n",
        "        word_list (list): A list of words (tokens).\n",
        "        model (Word2Vec model): The trained Word2Vec model.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The averaged word vector for the document.\n",
        "                       Returns a zero vector if no words are in the vocabulary.\n",
        "    \"\"\"\n",
        "    # Remove words not in the vocabulary\n",
        "    words_in_vocab = [word for word in word_list if word in model.wv.index_to_key]\n",
        "\n",
        "    if not words_in_vocab:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    # Average the word vectors\n",
        "    return np.mean(model.wv[words_in_vocab], axis=0)\n",
        "\n",
        "# Create document vectors for the preprocessed text\n",
        "X_word2vec = np.array([document_vector(text.split(), word2vec_model) for text in new_df['preprocessed_text']])\n",
        "\n",
        "# Display the shape of the resulting Word2Vec vectors\n",
        "print(\"Shape of Word2Vec vectors:\", X_word2vec.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c11bb85"
      },
      "source": [
        "## Train, Evaluate, and Fine-tune with Word2Vec Vectors\n",
        "\n",
        "### Subtask:\n",
        "Train the SGDClassifier model using Word2Vec features, evaluate its performance, and fine-tune it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ef4a2c"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the data using the Word2Vec features and the dialect labels. Train an SGDClassifier model on the training data, evaluate it on the testing data, and then perform hyperparameter tuning using GridSearchCV to find the best parameters for the model with Word2Vec features. Finally, evaluate the best model and print the classification report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08a268c4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_word2vec and new_df['dialect'] are available from previous successful cells\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = X_word2vec\n",
        "y = new_df['dialect']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate SGDClassifier\n",
        "sgd_model_w2v = SGDClassifier(loss='hinge', penalty='l2', random_state=42)\n",
        "\n",
        "# Train the SGDClassifier model\n",
        "sgd_model_w2v.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the initial model\n",
        "y_pred_w2v = sgd_model_w2v.predict(X_test)\n",
        "accuracy_w2v = accuracy_score(y_test, y_pred_w2v)\n",
        "print(f\"Accuracy of the initial SGD model with Word2Vec on the testing data: {accuracy_w2v}\")\n",
        "\n",
        "\n",
        "# Define the hyperparameter grid for SGDClassifier\n",
        "param_grid_w2v = {\n",
        "    'loss': ['hinge', 'modified_huber'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.0001, 0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search_w2v = GridSearchCV(SGDClassifier(random_state=42), param_grid_w2v, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search_w2v.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_sgd_model_w2v = grid_search_w2v.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the testing data\n",
        "y_pred_best_w2v = best_sgd_model_w2v.predict(X_test)\n",
        "\n",
        "# Print the best hyperparameters and the evaluation metrics\n",
        "print(\"\\nBest Hyperparameters with Word2Vec:\", grid_search_w2v.best_params_)\n",
        "print(\"\\nClassification Report on Test Set with Word2Vec:\")\n",
        "print(classification_report(y_test, y_pred_best_w2v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39ae3c1a"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "e1603320",
        "outputId": "9938f121-b13d-4661-afa5-6c16d5caab40"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the specified file path\n",
        "try:\n",
        "    df = pd.read_excel('/content/Main DataSet (5).xlsx')\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: '/content/Main DataSet (5).xlsx' not found. Please ensure the file is in the correct directory.\")\n",
        "    raise\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "display(df.head())\n",
        "\n",
        "# Display information about the DataFrame\n",
        "print(\"\\nDataFrame Information:\")\n",
        "display(df.info())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "First 5 rows of the DataFrame:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                            Promito                   Rajshahi  \\\n",
              "0                     তুমি কি করছো?              তুমি কী কইরছো   \n",
              "1              তুমি কোথা থেকে আসছো?         কোতি থেকে অ্যাসছো?   \n",
              "2              আপনি কোথায় যাচ্ছেন?               কতি জ্যাছেন?   \n",
              "3         আপনার দিনকাল কেমন যাচ্ছে?   দিন ক্যামন জ্যাছে আপনার?   \n",
              "4  আপনার সপ্তাহান্তের পরিকল্পনা কি?    সপ্তাহ শ্যাসে কি কইরবেন   \n",
              "\n",
              "                                 Sylhet  \\\n",
              "0                        তুমি কিতা করো?   \n",
              "1                      তুমি কইথাকি আইছ?   \n",
              "2                      আফনে কই যাইরাইন?   \n",
              "3                কিরম যায় বা তোমার দিন?   \n",
              "4  আফনার সামনের সাপ্তাত পরিকল্পনা কিতা?   \n",
              "\n",
              "                                   Chottogram  \\\n",
              "0                                 তুঁই কিরর ?   \n",
              "1                     তুঁই  হত্তুন আইয়্যির ?   \n",
              "2                                অনে হঁডে যর?   \n",
              "3                অঁনর দিনহাল ক্যান চলের ওয়া??   \n",
              "4  ত, শুক্কু-শনিবারে কি গরিবে চিন্তে গইজ্জু??   \n",
              "\n",
              "                              Rangpur        Bogura Dialect (বগুড়ার ভাষা)  \\\n",
              "0           কি কইরবান নাকছেন তোমরা???                 তুই ক্যা কত্ত্যাছিস?   \n",
              "1        কোনটে থাকি আসচেন বাহে তোমরা?             তুই কনটি থাকি আসত্যাছিস?   \n",
              "2               কোনটে জান বাহে তোমরা?               আঁরা কনটি যাত্ত্যাছেন?   \n",
              "3   বাহে আপনার দিনকাল ক্যামনে যাচ্ছে?      আঁরার দিনকাল ক্যামন যাত্ত্যাছে?   \n",
              "4  বাহে আপনার সপ্তাহের পরিকল্পনা কি?   আঁরার  সপ্তাহের শেষের পরিকল্পনা কি?   \n",
              "\n",
              "        Noakhali Dialect (নোয়াখাইল্লা)    Barishali Dialect (বরিশাইল্যা)  \n",
              "0                        তুঁই কিতা করর?                   তুমি কি করতেছো?  \n",
              "1                  তুঁই হোনডে ত্থন আইর?             তুমি কোথা দিয়া আইছো?  \n",
              "2                    আন্নে হোনডে যারেন?                  আপনি কোথায় যান?  \n",
              "3               আন্নের দিনকাল কেনে যার?        আপনার দিনকাল কেমন যাইতেছে?  \n",
              "4  আন্নের হপ্তাহর শেষের পরিকল্পনা কিতা?  আপনার শনি-রবিবারের পরিকল্পনা কি?  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a438089-e60a-4f10-8948-d901b74519ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Promito</th>\n",
              "      <th>Rajshahi</th>\n",
              "      <th>Sylhet</th>\n",
              "      <th>Chottogram</th>\n",
              "      <th>Rangpur</th>\n",
              "      <th>Bogura Dialect (বগুড়ার ভাষা)</th>\n",
              "      <th>Noakhali Dialect (নোয়াখাইল্লা)</th>\n",
              "      <th>Barishali Dialect (বরিশাইল্যা)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>তুমি কি করছো?</td>\n",
              "      <td>তুমি কী কইরছো</td>\n",
              "      <td>তুমি কিতা করো?</td>\n",
              "      <td>তুঁই কিরর ?</td>\n",
              "      <td>কি কইরবান নাকছেন তোমরা???</td>\n",
              "      <td>তুই ক্যা কত্ত্যাছিস?</td>\n",
              "      <td>তুঁই কিতা করর?</td>\n",
              "      <td>তুমি কি করতেছো?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>তুমি কোথা থেকে আসছো?</td>\n",
              "      <td>কোতি থেকে অ্যাসছো?</td>\n",
              "      <td>তুমি কইথাকি আইছ?</td>\n",
              "      <td>তুঁই  হত্তুন আইয়্যির ?</td>\n",
              "      <td>কোনটে থাকি আসচেন বাহে তোমরা?</td>\n",
              "      <td>তুই কনটি থাকি আসত্যাছিস?</td>\n",
              "      <td>তুঁই হোনডে ত্থন আইর?</td>\n",
              "      <td>তুমি কোথা দিয়া আইছো?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>আপনি কোথায় যাচ্ছেন?</td>\n",
              "      <td>কতি জ্যাছেন?</td>\n",
              "      <td>আফনে কই যাইরাইন?</td>\n",
              "      <td>অনে হঁডে যর?</td>\n",
              "      <td>কোনটে জান বাহে তোমরা?</td>\n",
              "      <td>আঁরা কনটি যাত্ত্যাছেন?</td>\n",
              "      <td>আন্নে হোনডে যারেন?</td>\n",
              "      <td>আপনি কোথায় যান?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>আপনার দিনকাল কেমন যাচ্ছে?</td>\n",
              "      <td>দিন ক্যামন জ্যাছে আপনার?</td>\n",
              "      <td>কিরম যায় বা তোমার দিন?</td>\n",
              "      <td>অঁনর দিনহাল ক্যান চলের ওয়া??</td>\n",
              "      <td>বাহে আপনার দিনকাল ক্যামনে যাচ্ছে?</td>\n",
              "      <td>আঁরার দিনকাল ক্যামন যাত্ত্যাছে?</td>\n",
              "      <td>আন্নের দিনকাল কেনে যার?</td>\n",
              "      <td>আপনার দিনকাল কেমন যাইতেছে?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>আপনার সপ্তাহান্তের পরিকল্পনা কি?</td>\n",
              "      <td>সপ্তাহ শ্যাসে কি কইরবেন</td>\n",
              "      <td>আফনার সামনের সাপ্তাত পরিকল্পনা কিতা?</td>\n",
              "      <td>ত, শুক্কু-শনিবারে কি গরিবে চিন্তে গইজ্জু??</td>\n",
              "      <td>বাহে আপনার সপ্তাহের পরিকল্পনা কি?</td>\n",
              "      <td>আঁরার  সপ্তাহের শেষের পরিকল্পনা কি?</td>\n",
              "      <td>আন্নের হপ্তাহর শেষের পরিকল্পনা কিতা?</td>\n",
              "      <td>আপনার শনি-রবিবারের পরিকল্পনা কি?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a438089-e60a-4f10-8948-d901b74519ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a438089-e60a-4f10-8948-d901b74519ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a438089-e60a-4f10-8948-d901b74519ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b93c0533-db71-4b9a-8b91-c0a1210fab6d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b93c0533-db71-4b9a-8b91-c0a1210fab6d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b93c0533-db71-4b9a-8b91-c0a1210fab6d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Promito\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u09ae\\u09bf \\u0995\\u09cb\\u09a5\\u09be \\u09a5\\u09c7\\u0995\\u09c7 \\u0986\\u09b8\\u099b\\u09cb?\",\n          \"\\u0986\\u09aa\\u09a8\\u09be\\u09b0 \\u09b8\\u09aa\\u09cd\\u09a4\\u09be\\u09b9\\u09be\\u09a8\\u09cd\\u09a4\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf?\",\n          \"\\u0986\\u09aa\\u09a8\\u09bf \\u0995\\u09cb\\u09a5\\u09be\\u09af\\u09bc \\u09af\\u09be\\u099a\\u09cd\\u099b\\u09c7\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rajshahi\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u0995\\u09cb\\u09a4\\u09bf \\u09a5\\u09c7\\u0995\\u09c7 \\u0985\\u09cd\\u09af\\u09be\\u09b8\\u099b\\u09cb?\",\n          \"\\u09b8\\u09aa\\u09cd\\u09a4\\u09be\\u09b9 \\u09b6\\u09cd\\u09af\\u09be\\u09b8\\u09c7 \\u0995\\u09bf \\u0995\\u0987\\u09b0\\u09ac\\u09c7\\u09a8\",\n          \"\\u0995\\u09a4\\u09bf \\u099c\\u09cd\\u09af\\u09be\\u099b\\u09c7\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sylhet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u09ae\\u09bf \\u0995\\u0987\\u09a5\\u09be\\u0995\\u09bf \\u0986\\u0987\\u099b?\",\n          \"\\u0986\\u09ab\\u09a8\\u09be\\u09b0 \\u09b8\\u09be\\u09ae\\u09a8\\u09c7\\u09b0 \\u09b8\\u09be\\u09aa\\u09cd\\u09a4\\u09be\\u09a4 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf\\u09a4\\u09be?\",\n          \"\\u0986\\u09ab\\u09a8\\u09c7 \\u0995\\u0987 \\u09af\\u09be\\u0987\\u09b0\\u09be\\u0987\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Chottogram\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u0981\\u0987  \\u09b9\\u09a4\\u09cd\\u09a4\\u09c1\\u09a8 \\u0986\\u0987\\u09af\\u09bc\\u09cd\\u09af\\u09bf\\u09b0 ?\",\n          \"\\u09a4, \\u09b6\\u09c1\\u0995\\u09cd\\u0995\\u09c1-\\u09b6\\u09a8\\u09bf\\u09ac\\u09be\\u09b0\\u09c7 \\u0995\\u09bf \\u0997\\u09b0\\u09bf\\u09ac\\u09c7 \\u099a\\u09bf\\u09a8\\u09cd\\u09a4\\u09c7 \\u0997\\u0987\\u099c\\u09cd\\u099c\\u09c1??\",\n          \"\\u0985\\u09a8\\u09c7 \\u09b9\\u0981\\u09a1\\u09c7 \\u09af\\u09b0?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rangpur\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u0995\\u09cb\\u09a8\\u099f\\u09c7 \\u09a5\\u09be\\u0995\\u09bf \\u0986\\u09b8\\u099a\\u09c7\\u09a8 \\u09ac\\u09be\\u09b9\\u09c7 \\u09a4\\u09cb\\u09ae\\u09b0\\u09be?\",\n          \"\\u09ac\\u09be\\u09b9\\u09c7 \\u0986\\u09aa\\u09a8\\u09be\\u09b0 \\u09b8\\u09aa\\u09cd\\u09a4\\u09be\\u09b9\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf? \",\n          \"\\u0995\\u09cb\\u09a8\\u099f\\u09c7 \\u099c\\u09be\\u09a8 \\u09ac\\u09be\\u09b9\\u09c7 \\u09a4\\u09cb\\u09ae\\u09b0\\u09be?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bogura Dialect (\\u09ac\\u0997\\u09c1\\u09a1\\u09bc\\u09be\\u09b0 \\u09ad\\u09be\\u09b7\\u09be)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u0987 \\u0995\\u09a8\\u099f\\u09bf \\u09a5\\u09be\\u0995\\u09bf \\u0986\\u09b8\\u09a4\\u09cd\\u09af\\u09be\\u099b\\u09bf\\u09b8?\",\n          \"\\u0986\\u0981\\u09b0\\u09be\\u09b0  \\u09b8\\u09aa\\u09cd\\u09a4\\u09be\\u09b9\\u09c7\\u09b0 \\u09b6\\u09c7\\u09b7\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf?\",\n          \"\\u0986\\u0981\\u09b0\\u09be \\u0995\\u09a8\\u099f\\u09bf \\u09af\\u09be\\u09a4\\u09cd\\u09a4\\u09cd\\u09af\\u09be\\u099b\\u09c7\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Noakhali Dialect (\\u09a8\\u09cb\\u09af\\u09bc\\u09be\\u0996\\u09be\\u0987\\u09b2\\u09cd\\u09b2\\u09be)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u0981\\u0987 \\u09b9\\u09cb\\u09a8\\u09a1\\u09c7 \\u09a4\\u09cd\\u09a5\\u09a8 \\u0986\\u0987\\u09b0?\",\n          \"\\u0986\\u09a8\\u09cd\\u09a8\\u09c7\\u09b0 \\u09b9\\u09aa\\u09cd\\u09a4\\u09be\\u09b9\\u09b0 \\u09b6\\u09c7\\u09b7\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf\\u09a4\\u09be?\",\n          \"\\u0986\\u09a8\\u09cd\\u09a8\\u09c7 \\u09b9\\u09cb\\u09a8\\u09a1\\u09c7 \\u09af\\u09be\\u09b0\\u09c7\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Barishali Dialect (\\u09ac\\u09b0\\u09bf\\u09b6\\u09be\\u0987\\u09b2\\u09cd\\u09af\\u09be)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u09ae\\u09bf \\u0995\\u09cb\\u09a5\\u09be \\u09a6\\u09bf\\u09af\\u09bc\\u09be \\u0986\\u0987\\u099b\\u09cb?\",\n          \"\\u0986\\u09aa\\u09a8\\u09be\\u09b0 \\u09b6\\u09a8\\u09bf-\\u09b0\\u09ac\\u09bf\\u09ac\\u09be\\u09b0\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf?\",\n          \"\\u0986\\u09aa\\u09a8\\u09bf \\u0995\\u09cb\\u09a5\\u09be\\u09af\\u09bc \\u09af\\u09be\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2113 entries, 0 to 2112\n",
            "Data columns (total 8 columns):\n",
            " #   Column                           Non-Null Count  Dtype \n",
            "---  ------                           --------------  ----- \n",
            " 0   Promito                          2113 non-null   object\n",
            " 1   Rajshahi                         2113 non-null   object\n",
            " 2   Sylhet                           2113 non-null   object\n",
            " 3   Chottogram                       2113 non-null   object\n",
            " 4   Rangpur                          2113 non-null   object\n",
            " 5   Bogura Dialect (বগুড়ার ভাষা)    2113 non-null   object\n",
            " 6   Noakhali Dialect (নোয়াখাইল্লা)  2113 non-null   object\n",
            " 7   Barishali Dialect (বরিশাইল্যা)   2113 non-null   object\n",
            "dtypes: object(8)\n",
            "memory usage: 132.2+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d615091"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "### Subtask:\n",
        "Clean and prepare the text data for model training. This may involve tokenization, removing stop words, and other text normalization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cd6905b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for text preprocessing and define a function to clean and preprocess the text data. Then apply the function to the 'text' column of the dataframe and display the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cd8f4a93",
        "outputId": "5e778977-c0c6-44ed-812a-62548c3ad6b0"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK data (only need to run this once)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.word_tokenize(\"hello world\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses text data.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text string.\n",
        "    \"\"\"\n",
        "    # Ensure the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\" # Return empty string for non-string inputs\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words (NLTK English stop words might not be effective for Bengali)\n",
        "    # stop_words = set(stopwords.words('english'))\n",
        "    # tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming (PorterStemmer is for English, will not work for Bengali)\n",
        "    # stemmer = PorterStemmer()\n",
        "    # tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Select a target dialect column (e.g., 'Promito' as the base dialect)\n",
        "# You can choose a different column as your target based on your classification goal\n",
        "target_dialect = 'Promito'\n",
        "\n",
        "# Create a new dataframe with 'text' and 'dialect' columns\n",
        "# Combine text from all dialect columns and create corresponding labels\n",
        "data = []\n",
        "for col in df.columns:\n",
        "    # Assuming each column name is the dialect label\n",
        "    dialect_label = col\n",
        "    for text in df[col]:\n",
        "        data.append({'text': text, 'dialect': dialect_label})\n",
        "\n",
        "new_df = pd.DataFrame(data)\n",
        "\n",
        "# Apply the preprocessing function to the 'text' column\n",
        "new_df['preprocessed_text'] = new_df['text'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Display the first few rows with the new columns\n",
        "display(new_df.head())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                               text  dialect                preprocessed_text\n",
              "0                     তুমি কি করছো?  Promito                     তুমি কি করছো\n",
              "1              তুমি কোথা থেকে আসছো?  Promito              তুমি কোথা থেকে আসছো\n",
              "2              আপনি কোথায় যাচ্ছেন?  Promito              আপনি কোথায় যাচ্ছেন\n",
              "3         আপনার দিনকাল কেমন যাচ্ছে?  Promito         আপনার দিনকাল কেমন যাচ্ছে\n",
              "4  আপনার সপ্তাহান্তের পরিকল্পনা কি?  Promito  আপনার সপ্তাহান্তের পরিকল্পনা কি"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb853c6e-be77-43d5-8f57-4d234ca11a67\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>dialect</th>\n",
              "      <th>preprocessed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>তুমি কি করছো?</td>\n",
              "      <td>Promito</td>\n",
              "      <td>তুমি কি করছো</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>তুমি কোথা থেকে আসছো?</td>\n",
              "      <td>Promito</td>\n",
              "      <td>তুমি কোথা থেকে আসছো</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>আপনি কোথায় যাচ্ছেন?</td>\n",
              "      <td>Promito</td>\n",
              "      <td>আপনি কোথায় যাচ্ছেন</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>আপনার দিনকাল কেমন যাচ্ছে?</td>\n",
              "      <td>Promito</td>\n",
              "      <td>আপনার দিনকাল কেমন যাচ্ছে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>আপনার সপ্তাহান্তের পরিকল্পনা কি?</td>\n",
              "      <td>Promito</td>\n",
              "      <td>আপনার সপ্তাহান্তের পরিকল্পনা কি</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb853c6e-be77-43d5-8f57-4d234ca11a67')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eb853c6e-be77-43d5-8f57-4d234ca11a67 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eb853c6e-be77-43d5-8f57-4d234ca11a67');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0c7bbca4-0ba3-49c7-986d-ca1dd14dcaf8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c7bbca4-0ba3-49c7-986d-ca1dd14dcaf8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0c7bbca4-0ba3-49c7-986d-ca1dd14dcaf8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(new_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u09ae\\u09bf \\u0995\\u09cb\\u09a5\\u09be \\u09a5\\u09c7\\u0995\\u09c7 \\u0986\\u09b8\\u099b\\u09cb?\",\n          \"\\u0986\\u09aa\\u09a8\\u09be\\u09b0 \\u09b8\\u09aa\\u09cd\\u09a4\\u09be\\u09b9\\u09be\\u09a8\\u09cd\\u09a4\\u09c7\\u09b0 \\u09aa\\u09b0\\u09bf\\u0995\\u09b2\\u09cd\\u09aa\\u09a8\\u09be \\u0995\\u09bf?\",\n          \"\\u0986\\u09aa\\u09a8\\u09bf \\u0995\\u09cb\\u09a5\\u09be\\u09af\\u09bc \\u09af\\u09be\\u099a\\u09cd\\u099b\\u09c7\\u09a8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dialect\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Promito\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"preprocessed_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09a4\\u09c1\\u09ae\\u09bf \\u0995\\u09cb\\u09a5\\u09be \\u09a5\\u09c7\\u0995\\u09c7 \\u0986\\u09b8\\u099b\\u09cb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0088bb5e"
      },
      "source": [
        "## Split the data\n",
        "\n",
        "### Subtask:\n",
        "Split the dataset into training and testing sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c52ec3f"
      },
      "source": [
        "**Reasoning**:\n",
        "The goal is to split the data into training and testing sets. This involves defining features and target variables and then using `train_test_split` to perform the split. Finally, the shapes of the resulting sets are printed to confirm the split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fe81139",
        "outputId": "c42dd012-e78a-4ea8-bfe8-fbed6467c097"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = new_df['preprocessed_text']\n",
        "y = new_df['dialect']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (13523,)\n",
            "Shape of X_test: (3381,)\n",
            "Shape of y_train: (13523,)\n",
            "Shape of y_test: (3381,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "877a39b9"
      },
      "source": [
        "## Choose and train a model\n",
        "\n",
        "### Subtask:\n",
        "Select an appropriate classification model (e.g., SGDClassifier) and train it on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17aa4c98"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply TF-IDF vectorization to the training and testing data to convert the text into numerical features. Then, instantiate and train an SGDClassifier model on the TF-IDF transformed training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2055bcc2",
        "outputId": "c9e84a84-a2df-441d-ebd4-807d6f07f9d5"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Instantiate TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform training data, transform testing data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate SGDClassifier\n",
        "sgd_model = SGDClassifier(loss='hinge', penalty='l2', random_state=42)\n",
        "\n",
        "# Train the SGDClassifier model\n",
        "sgd_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"TF-IDF Vectorization and SGDClassifier training complete.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vectorization and SGDClassifier training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1a4f0a"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model on the testing data to assess its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a405893b"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained model on the testing data to assess its performance by making predictions and calculating the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfe985ce",
        "outputId": "081d0b94-1458-42f8-c340-f422920a4288"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Use the trained sgd_model to make predictions on the X_test_tfidf data\n",
        "y_pred = sgd_model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate the accuracy of the model's predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the calculated accuracy score\n",
        "print(f\"Accuracy of the SGD model on the testing data: {accuracy}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SGD model on the testing data: 0.3460514640638864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5acb5825"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "### Subtask:\n",
        "Optimize the model's hyperparameters or explore different models to improve accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "171f4eeb"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for hyperparameter tuning and evaluation, define the parameter grid, instantiate and fit GridSearchCV, retrieve the best model, evaluate it, and print the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98092b2b",
        "outputId": "6e7ae3af-ffc0-46c9-8521-0677edadafae"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define the hyperparameter grid for SGDClassifier\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'modified_huber'], # 'log' was removed as it can cause convergence issues with sparse data and SGD\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.0001, 0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(SGDClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_sgd_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the testing data\n",
        "y_pred_best = best_sgd_model.predict(X_test_tfidf)\n",
        "\n",
        "# Print the best hyperparameters and the evaluation metrics\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred_best))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'alpha': 0.001, 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
            "\n",
            "Classification Report on Test Set:\n",
            "                                 precision    recall  f1-score   support\n",
            "\n",
            " Barishali Dialect (বরিশাইল্যা)       0.50      0.40      0.44       410\n",
            "  Bogura Dialect (বগুড়ার ভাষা)       0.29      0.31      0.30       421\n",
            "                     Chottogram       0.43      0.43      0.43       435\n",
            "Noakhali Dialect (নোয়াখাইল্লা)       0.40      0.32      0.36       432\n",
            "                        Promito       0.38      0.28      0.32       403\n",
            "                       Rajshahi       0.22      0.39      0.28       426\n",
            "                        Rangpur       0.31      0.32      0.32       433\n",
            "                         Sylhet       0.58      0.46      0.52       421\n",
            "\n",
            "                       accuracy                           0.36      3381\n",
            "                      macro avg       0.39      0.36      0.37      3381\n",
            "                   weighted avg       0.39      0.36      0.37      3381\n",
            "\n"
          ]
        }
      ]
    }
  ]
}